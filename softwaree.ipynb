{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a545fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vit_xgb.py\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch / torchvision for ViT\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Model / training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00261060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58501c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "DATA_DIR = \"\"   # path where zip was extracted\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_EXTRACT = 32                    \n",
    "IMG_SIZE = 224                        \n",
    "RANDOM_STATE = 42\n",
    "XGB_MODEL_PATH = \"xgb_vit_model.joblib\"\n",
    "SCALER_PATH = \"scaler.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b676f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse \n",
    "\n",
    "LABEL_RE = re.compile(r\".*\\((\\d+)\\s*-\\s*(\\d+)\\).*\")  \n",
    "def parse_label_from_folder(foldername):\n",
    "    \"\"\"\n",
    "    Input: foldername string, e.g. \"Apple(1-5)\" or \"Banana (2-7)\"\n",
    "    Returns: float midpoint label, e.g. 3.0\n",
    "    Raises ValueError if pattern not found.\n",
    "    \"\"\"\n",
    "    m = LABEL_RE.match(foldername)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Folder name '{foldername}' does not match pattern 'name(min-max)'.\")\n",
    "    a = int(m.group(1))\n",
    "    b = int(m.group(2))\n",
    "    return (a + b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list\n",
    "\n",
    "def collect_image_paths_and_labels(root_dir):\n",
    "    root = Path(root_dir)\n",
    "    rows = []\n",
    "    for sub in sorted(root.iterdir()):\n",
    "        if not sub.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            label = parse_label_from_folder(sub.name)\n",
    "        except ValueError:\n",
    "            # skip folders that don't follow pattern\n",
    "            print(f\"Skipping folder (cannot parse label): {sub}\")\n",
    "            continue\n",
    "        # collect image files under this folder (non-recursive)\n",
    "        img_files = [p for p in sub.rglob(\"*\") if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}]\n",
    "        if not img_files:\n",
    "            print(f\"No images found in {sub}, skipping.\")\n",
    "            continue\n",
    "        for p in img_files:\n",
    "            rows.append((str(p), float(label)))\n",
    "    df = pd.DataFrame(rows, columns=[\"path\", \"label\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf29a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ViT\n",
    "\n",
    "def build_vit_feature_extractor(device=DEVICE):\n",
    "    # Try torchvision.models.vit_b_16 (available in torchvision >= 0.12+)\n",
    "    try:\n",
    "        from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"torchvision ViT model not available in this environment. \"\n",
    "                           \"Ensure torchvision >= 0.12 or install timm and adjust code to use timm.models.vit_base_patch16_224.\")\n",
    "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "    vit = vit_b_16(weights=weights).to(device)\n",
    "    vit.eval()\n",
    "    # Remove the classification head: get the representation before final head.\n",
    "    # torchvision's vit has vit.heads or classifier attribute; we'll create a model that outputs the embedding.\n",
    "    # Implementation detail depends on torchvision version; this approach uses forward to get features from vit.encoder\n",
    "    class ViTFeatureExtractor(nn.Module):\n",
    "        def __init__(self, vit_model):\n",
    "            super().__init__()\n",
    "            self.vit = vit_model\n",
    "            # For newer torchvision the attribute for head is 'heads' or 'head'\n",
    "            # We'll rely on vit.forward returning logits; instead we replicate forward up to pre_logits.\n",
    "        def forward(self, x):\n",
    "            # Use vit._process_input & vit.encoder if available.\n",
    "            # Simpler: call vit.forward_features if available (returns embeddings). Check attribute:\n",
    "            if hasattr(self.vit, \"forward_features\"):\n",
    "                feats = self.vit.forward_features(x)  # shape (B, C)\n",
    "            else:\n",
    "                # fallback: run forward and remove head - but that may compute logits; hope forward_features exists\n",
    "                feats = self.vit(x)\n",
    "            return feats\n",
    "    feat_model = ViTFeatureExtractor(vit).to(device)\n",
    "    return feat_model, weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "def extract_features_dataframe(df, feature_model, preprocess_transform, batch_size=BATCH_EXTRACT, device=DEVICE):\n",
    "    n = len(df)\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    feature_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n, batch_size), desc=\"Extracting features\"):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "            imgs = []\n",
    "            for p in batch[\"path\"]:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                img = preprocess_transform(img)  \n",
    "                imgs.append(img)\n",
    "            imgs = torch.stack(imgs, dim=0).to(device)\n",
    "            feats = feature_model(imgs)  \n",
    "            if isinstance(feats, torch.Tensor):\n",
    "                feats = feats.cpu().numpy()\n",
    "            X_list.append(feats)\n",
    "            y_list.extend(batch[\"label\"].values.tolist())\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.array(y_list, dtype=float)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e67e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval\n",
    "\n",
    "def train_xgb(X_train, y_train, X_val, y_val, random_state=RANDOM_STATE):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"eta\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"seed\": random_state,\n",
    "        \"verbosity\": 1,\n",
    "    }\n",
    "    watchlist = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=20,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c944b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    dmat = xgb.DMatrix(X)\n",
    "    preds = model.predict(dmat)\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    rmse = root_mean_squared_error(y, preds)\n",
    "    r2 = r2_score(y, preds)\n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2, \"preds\": preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95c6c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_sklearn_model(model, X_train, y_train, X_test, y_test, name=\"Model\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = root_mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} -> MAE: {mae:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
    "    return model, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bed832fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image paths and labels...\n",
      "Skipping folder (cannot parse label): C:\\Users\\kiera\\Downloads\\archive\\Expired\n",
      "Found 1488 images across 13 folders.\n",
      "                                                path  label\n",
      "0  C:\\Users\\kiera\\Downloads\\archive\\Apple(1-5)\\fr...    3.0\n",
      "1  C:\\Users\\kiera\\Downloads\\archive\\Apple(1-5)\\fr...    3.0\n",
      "2  C:\\Users\\kiera\\Downloads\\archive\\Apple(1-5)\\fr...    3.0\n",
      "3  C:\\Users\\kiera\\Downloads\\archive\\Apple(1-5)\\fr...    3.0\n",
      "4  C:\\Users\\kiera\\Downloads\\archive\\Apple(1-5)\\fr...    3.0\n",
      "Train: 1190  Test: 298\n",
      "Building ViT feature extractor...\n",
      "Extracting train features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 38/38 [05:42<00:00,  9.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 10/10 [01:18<00:00,  7.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost regressor...\n",
      "[0]\ttrain-rmse:4.72557\tval-rmse:4.57686\n",
      "[20]\ttrain-rmse:1.96488\tval-rmse:2.17577\n",
      "[40]\ttrain-rmse:0.98106\tval-rmse:1.48718\n",
      "[60]\ttrain-rmse:0.66277\tval-rmse:1.32779\n",
      "[80]\ttrain-rmse:0.57234\tval-rmse:1.29422\n",
      "[100]\ttrain-rmse:0.54768\tval-rmse:1.29292\n",
      "[120]\ttrain-rmse:0.53872\tval-rmse:1.29656\n",
      "[140]\ttrain-rmse:0.53187\tval-rmse:1.29288\n",
      "[160]\ttrain-rmse:0.52757\tval-rmse:1.29398\n",
      "[180]\ttrain-rmse:0.52569\tval-rmse:1.29744\n",
      "[185]\ttrain-rmse:0.52544\tval-rmse:1.29578\n",
      "Evaluating on test set...\n",
      "Test MAE: 0.6946994272254458\n",
      "Test RMSE: 1.295782436688327\n",
      "Test R2: 0.9261411211066203\n",
      "Saved XGBoost model to xgb_vit_model.joblib and scaler to scaler.joblib.\n",
      "Sample predictions on test images:\n",
      "C:\\Users\\kiera\\Downloads\\archive\\Banana(1-5)\\frame5360.jpg -> predicted: 4.439, true midpoint: 3.0\n",
      "C:\\Users\\kiera\\Downloads\\archive\\Apple(1-5)\\frame435.jpg -> predicted: 3.018, true midpoint: 3.0\n",
      "C:\\Users\\kiera\\Downloads\\archive\\Apple(5-10)\\frame340.jpg -> predicted: 7.698, true midpoint: 7.5\n",
      "C:\\Users\\kiera\\Downloads\\archive\\Carrot(3-4)\\frame180 (2).jpg -> predicted: 3.251, true midpoint: 3.5\n",
      "C:\\Users\\kiera\\Downloads\\archive\\Tomato(10-15)\\frame160.jpg -> predicted: 12.271, true midpoint: 12.5\n",
      "Random Forest -> MAE: 0.736, RMSE: 1.257, R2: 0.931\n",
      "Ridge Regression -> MAE: 0.929, RMSE: 1.476, R2: 0.904\n",
      "KNN Regressor -> MAE: 0.471, RMSE: 1.118, R2: 0.945\n",
      "All models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Collecting image paths and labels...\")\n",
    "    df = collect_image_paths_and_labels(DATA_DIR)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"No images found under {DATA_DIR}. Check path.\")\n",
    "    print(f\"Found {len(df)} images across {df.path.apply(lambda p: Path(p).parent.name).nunique()} folders.\")\n",
    "    print(df.head())\n",
    "\n",
    "    # split\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    print(f\"Train: {len(train_df)}  Test: {len(test_df)}\")\n",
    "\n",
    "    print(\"Building ViT feature extractor...\")\n",
    "    feat_model, preprocess_transform = build_vit_feature_extractor()\n",
    "    print(\"Extracting train features...\")\n",
    "    X_train, y_train = extract_features_dataframe(train_df.reset_index(drop=True), feat_model, preprocess_transform)\n",
    "    print(\"Extracting test features...\")\n",
    "    X_test, y_test = extract_features_dataframe(test_df.reset_index(drop=True), feat_model, preprocess_transform)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Training XGBoost regressor...\")\n",
    "    xgb_model = train_xgb(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    print(\"Evaluating on test set...\")\n",
    "    eval_res = evaluate_model(xgb_model, X_test_scaled, y_test)\n",
    "    print(\"Test MAE:\", eval_res[\"mae\"])\n",
    "    print(\"Test RMSE:\", eval_res[\"rmse\"])\n",
    "    print(\"Test R2:\", eval_res[\"r2\"])\n",
    "\n",
    "    joblib.dump(xgb_model, XGB_MODEL_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"Saved XGBoost model to {XGB_MODEL_PATH} and scaler to {SCALER_PATH}.\")\n",
    "\n",
    "    def predict_shelf_life(image_path):\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_t = preprocess_transform(img).unsqueeze(0).to(DEVICE)  # (1,C,H,W)\n",
    "        feat = feat_model(img_t)\n",
    "        if isinstance(feat, torch.Tensor):\n",
    "            feat = feat.detach().cpu().numpy()\n",
    "        feat_scaled = scaler.transform(feat)\n",
    "        dmat = xgb.DMatrix(feat_scaled)\n",
    "        pred = xgb_model.predict(dmat)[0]\n",
    "        return float(pred)\n",
    "\n",
    "    sample_paths = test_df[\"path\"].sample(min(5, len(test_df)), random_state=RANDOM_STATE).tolist()\n",
    "    print(\"Sample predictions on test images:\")\n",
    "    for p in sample_paths:\n",
    "        pred = predict_shelf_life(p)\n",
    "        true = float(test_df.loc[test_df[\"path\"] == p, \"label\"].iloc[0])\n",
    "        print(f\"{p} -> predicted: {pred:.3f}, true midpoint: {true}\")\n",
    "\n",
    "    rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=RANDOM_STATE)\n",
    "    rf_model, rf_preds = train_and_evaluate_sklearn_model(rf_model, X_train_scaled, y_train, X_test_scaled, y_test, \"Random Forest\")\n",
    "    joblib.dump(rf_model, \"rf_vit_model.joblib\")\n",
    "\n",
    "\n",
    "    ridge_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "    ridge_model, ridge_preds = train_and_evaluate_sklearn_model(ridge_model, X_train_scaled, y_train, X_test_scaled, y_test, \"Ridge Regression\")\n",
    "    joblib.dump(ridge_model, \"ridge_vit_model.joblib\")\n",
    "\n",
    "\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "    knn_model, knn_preds = train_and_evaluate_sklearn_model(knn_model, X_train_scaled, y_train, X_test_scaled, y_test, \"KNN Regressor\")\n",
    "    joblib.dump(knn_model, \"knn_vit_model.joblib\")\n",
    "\n",
    "    print(\"All models trained and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3630e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scaler loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "XGB_MODEL_PATH = \"xgb_vit_model.joblib\"\n",
    "SCALER_PATH = \"scaler.joblib\"\n",
    "\n",
    "xgb_model = joblib.load(XGB_MODEL_PATH) #insert model path\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "print(\"Model and scaler loaded successfully.\")\n",
    "\n",
    "feat_model, preprocess_transform = build_vit_feature_extractor()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "feat_model.to(DEVICE)\n",
    "feat_model.eval()  \n",
    "\n",
    "def predict_shelf_life(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_t = preprocess_transform(img).unsqueeze(0).to(DEVICE)  \n",
    "    \n",
    "\n",
    "    feat = feat_model(img_t)\n",
    "    if isinstance(feat, torch.Tensor):\n",
    "        feat = feat.detach().cpu().numpy()  \n",
    "    \n",
    "\n",
    "    feat_scaled = scaler.transform(feat)\n",
    "    \n",
    "\n",
    "    dmat = xgb.DMatrix(feat_scaled)\n",
    "    pred = xgb_model.predict(dmat)[0]\n",
    "    \n",
    "    return float(pred)\n",
    "\n",
    "new_image_path = \"path_to_your_image.jpg\"  # replace with actual image path\n",
    "predicted_shelf_life = predict_shelf_life(new_image_path)\n",
    "print(f\"Predicted shelf-life midpoint: {predicted_shelf_life:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb735d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
